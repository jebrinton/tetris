# Ohio
* Argparse in Python
* Stochastic hill climbing—assign weights to the objective function to each neighbor
    * Better neighbors—higher probabilities
* Newton-Raphson
    * First derivative equal to 0 are the significant points
    * Find the roots of the gradient/derivative (eq. = 0)
    * Hessian—remember from Calc III this is the matrix of the second derivative (gradient)
* "It turns out socialization is good for a lot of things"
* Beam search
    * $k$ states in memory
    * How NLP, translation work
    * Get $k$ hill climbers, but they can all go to any neighbor of the other hill climbers
    * (Choose the $k$ best neighbors)
    
    * "It turns out diversity is always a good thing"
    * Greater diversity = more perspectives on the objective surface
    * Random restarts on the hill climbers
    * Choose $k$ stochastically, not just the best $k$
* Genetic algorithms
    * Form of stochastic beam search
    * Requires "fitness function"
    * Child is made from two parents
    * Choose parents deterministically or probabilistically
    * Mutation probability

    * Schemas—some properties are fixed, others don't care about (st* gets all words that start with st)
    * Kinda like regular expressions
    * S is all possible schema individuals
    * I is all possible individuals
    * If the avg. objective value of S is greater than that of I, S is a good schema

    * Good sampling
    